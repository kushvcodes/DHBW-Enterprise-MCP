{
    "nav_title": "Navigation",
    "nav_app": "App",
    "nav_about": "About MCP",
    "nav_explorer": "Tool Explorer",
    "nav_create": "Create a Tool",
    "nav_model": "The Model in MCP",
    "nav_exercise": "Learning Trail Exercise",
    "nav_goto": "Go to",
    "settings_header": "Settings",
    "lang_label": "Language",

    "app_title": "ðŸ›ï¸ DHBW MCP Assistant",
    "app_welcome": "Welcome to the DHBW System. I can search for **Grades**, check **Schedules**, or issue **Certificates**. How can I assist?",
    "status_processing": "ðŸ§  Processing...",
    "status_connecting": "ðŸ”Œ Connecting to Enterprise Backend...",
    "status_routing": "ðŸ¤” Routing Request...",
    "status_executing": "ðŸš€ Executing Tool: `{tool_name}`",
    "status_fetching": "ðŸ“– Fetching Resource: `{uri}`",
    "status_formatting": "âœ¨ Formatting Response...",
    "status_complete": "âœ… Complete",
    "error_connect": "âŒ Connection Failed",
    "error_msg": "âš ï¸ **Error:** Could not connect to the MCP Server.",
    "metrics_title": "ðŸ› ï¸ Scientific Metrics (For Chapter 5)",
    "metric_latency": "â±ï¸ Tool Latency",
    "metric_overhead": "ðŸ“¦ Protocol Overhead",
    "metric_cost": "ðŸ’° Cost (DeepSeek)",
    "metric_tax_caption": "Context Tax: {ratio:.1f}x (Input/Output)",
    "metric_cost_caption": "vs GPT-4o: ${cost:.6f}",
    "insight": "**Seminar Insight:** The 'Context Tax' shows we sent {input} tokens of tool definitions to get {output} tokens of data. DeepSeek reduces this cost impact by ~{ratio:.1f}x compared to GPT-4o.",

    "about_title": "About the Model Context Protocol (MCP)",
    "about_intro": "This prototype is a demonstration of the **Model Context Protocol (MCP)**.",
    "about_desc": "MCP is a protocol that allows Large Language Models (LLMs) to interact with external tools and data sources in a standardized way. This enables developers to build powerful applications where LLMs can access real-time information and perform actions.",
    "about_how_title": "### How it works",
    "about_step1": "1. **MCP Server:** A server exposes 'tools' (functions) and 'resources' (data).",
    "about_step2": "2. **MCP Client:** This app connects to the server and makes tools known to the LLM.",
    "about_step3": "3. **LLM Interaction:** The LLM decides to use tools to fulfill user requests.",
    "about_step4": "4. **Response:** The LLM uses the data to generate a response.",

    "explorer_title": "Interactive Tool Explorer",
    "explorer_intro": "Here you can see the tools available from the MCP server.",
    "explorer_error": "Could not fetch tools. Is the server running?",
    "tool_name": "Tool Name",
    "tool_desc": "Description",
    
    "tutorial_title": "Guided Tutorial: Create Your First Tool",
    "tutorial_intro": "This tutorial will walk you through creating a new MCP tool from scratch.",
    "tutorial_step1_title": "Step 1: Add Data to `db.json`",
    "tutorial_step1_desc": "First, let's add some new data to our database file, `src/db.json`. We'll add a list of university events.",
    "tutorial_step1_code_desc": "Open `src/db.json` and add the following `events` section to the JSON object:",
    "tutorial_step2_title": "Step 2: Define the Schema in `src/schema.ts`",
    "tutorial_step2_desc": "Next, we need to define the input schema for our new tool. This tells the system what kind of arguments our tool accepts.",
    "tutorial_step2_code_desc": "Open `src/schema.ts` and add the following schema definition:",
    "tutorial_step3_title": "Step 3: Add the Tool Logic in `src/index.ts`",
    "tutorial_step3_desc": "Now for the core logic. We'll define the tool itself, which reads the event data from our `db.json` file.",
    "tutorial_step3_code_desc": "Open `src/index.ts` and add the following tool definition alongside the others:",
    "tutorial_step4_title": "Step 4: Restart and Test Your Tool",
    "tutorial_step4_desc": "To apply the changes, you need to restart the MCP server. Stop the server (Ctrl+C in the terminal where you ran `npm start`) and start it again.",
    "tutorial_step4_info": "Make sure you have saved your changes to all three files before restarting the server.",
    "tutorial_step4_test_desc": "Once the server is running again, you can test your new tool here!",
    "tutorial_btn_text": "Test 'get_events' Tool",
    "tutorial_btn_success": "Successfully called 'get_events' tool!",
    "tutorial_btn_warning": "The 'get_events' tool was not found on the server. Did you restart it after saving your changes?",
    "tutorial_error": "An error occurred: {e}",

    "model_title": "Understanding the 'Model' in MCP",
    "model_intro": "The 'Model' in Model Context Protocol refers to a Large Language Model (LLM), like Google's Gemini or DeepSeek. The LLM acts as the 'brain' of the application, deciding when to use the tools provided by the MCP server to answer a user's query.",
    "model_fc_title": "The Power of Function Calling",
    "model_fc_desc": "Modern LLMs have a powerful feature called **function calling** (or 'tool use'). This allows a developer to describe a set of functions (our MCP tools) to the LLM. When the user asks a question, the LLM can analyze the request and, if it deems a function is necessary to answer it, it will output a special 'function call' object. This object contains: The `name` of the function it wants to call. The `arguments` it wants to pass to that function. Our application code receives this object, executes the corresponding tool on the MCP server, and sends the result back to the LLM. The LLM then uses this result to formulate its final, data-informed answer to the user.",
    "model_app_approach": "Approach 1: The Two-Step Prompt (Python Client)",
    "model_app_desc": "The main application in this prototype (`client/frontend.py`) uses a 'two-step' prompting strategy. This is a common and effective pattern when using models that might not have robust, built-in function calling, or when you want more control over the routing logic.",
    "model_router_title": "Step 1: The Router",
    "model_router_desc": "First, we send a 'Router Prompt' to the LLM. This prompt describes the available tools and asks the LLM to decide which one to use based on the user's query. It is instructed to respond *only* with a JSON object representing its decision.",
    "model_formatter_title": "Step 2: The Formatter",
    "model_formatter_desc": "After executing the tool and getting the data, we send a *second* prompt to the LLM. This 'Formatter Prompt' includes the original query and the data we just fetched. It asks the LLM to formulate a user-friendly answer and, importantly, to suggest a 'learning trail' next step.",
    "model_ts_approach": "Approach 2: Native Function Calling (TypeScript Client)",
    "model_ts_desc": "The `src/client.ts` file demonstrates a more direct approach using the Google Gemini API's native function calling feature. This approach is often cleaner and requires less prompt engineering. In this method, we provide the tool schemas directly to the model when we initialize the chat session. The model then handles the decision-making process internally.",
    "model_info_note": "Both approaches are valid and have their own pros and cons. The two-step approach gives you more fine-grained control, while the native function calling approach is often simpler to implement.",

    "exercise_title": "Exercise: Extend the Learning Trail",
    "exercise_intro": "The 'Learning Trail' is the feature where the assistant suggests the next logical step to the user.",
    "exercise_current_prompt": "Current Formatter Prompt",
    "exercise_your_turn": "Your Turn!",
    "exercise_task": "Now, let's try to extend the learning trail. For example, what if the user just used the `get_events` tool you created? What would be a good next step to suggest? Modify the prompt below. You can add a new rule to the `LEARNING TRAIL` section. For example: `- If they checked events -> \"Would you like to book a ticket?\"`",
    "exercise_custom_prompt_label": "Your Custom Formatter Prompt:",
    "exercise_save_btn": "Save and Apply Custom Prompt",
    "exercise_save_success": "Your custom prompt has been saved! Now go to the 'App' page and try a query that would trigger your new rule.",
    "exercise_active_info": "A custom formatter prompt is currently active.",
    "exercise_reset_btn": "Reset to Default Prompt",
    "exercise_reset_success": "The formatter prompt has been reset to its default."
}
