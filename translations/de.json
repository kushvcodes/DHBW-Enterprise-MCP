{
    "nav_title": "Navigation",
    "nav_app": "App",
    "nav_about": "√úber MCP",
    "nav_explorer": "Tool-Explorer",
    "nav_create": "Tool Erstellen",
    "nav_model": "Das Modell in MCP",
    "nav_exercise": "Lernpfad-√úbung",
    "nav_goto": "Gehe zu",
    "settings_header": "Einstellungen",
    "lang_label": "Sprache",
    
    "app_title": "üèõÔ∏è DHBW MCP Assistent",
    "app_welcome": "Willkommen im DHBW-System. Ich kann nach **Noten**, **Stundenpl√§nen** oder **Zertifikaten** suchen. Wie kann ich helfen?",
    "status_processing": "üß† Verarbeite...",
    "status_connecting": "üîå Verbinde mit Enterprise-Backend...",
    "status_routing": "ü§î Routen der Anfrage...",
    "status_executing": "üöÄ F√ºhre Tool aus: `{tool_name}`",
    "status_fetching": "üìñ Lade Ressource: `{uri}`",
    "status_formatting": "‚ú® Formatiere Antwort...",
    "status_complete": "‚úÖ Fertig",
    "error_connect": "‚ùå Verbindung fehlgeschlagen",
    "error_msg": "‚ö†Ô∏è **Fehler:** Konnte nicht mit dem MCP-Server verbinden.",
    "metrics_title": "üõ†Ô∏è Wissenschaftliche Metriken (Kapitel 5)",
    "metric_latency": "‚è±Ô∏è Latenz",
    "metric_overhead": "üì¶ Protokoll-Overhead",
    "metric_cost": "üí∞ Kosten (DeepSeek)",
    "metric_tax_caption": "Context Tax: {ratio:.1f}x (Input/Output)",
    "metric_cost_caption": "vs GPT-4o: ${cost:.6f}",
    "insight": "**Seminar-Erkenntnis:** Der 'Context Tax' zeigt, dass wir {input} Token senden, um {output} Token Daten zu erhalten. DeepSeek reduziert die Kosten um ~{ratio:.1f}x im Vergleich zu GPT-4o.",
    
    "about_title": "√úber das Model Context Protocol (MCP)",
    "about_intro": "Dieser Prototyp ist eine Demonstration des **Model Context Protocol (MCP)**.",
    "about_desc": "MCP ist ein Protokoll, das es Large Language Models (LLMs) erm√∂glicht, auf standardisierte Weise mit externen Tools und Datenquellen zu interagieren. Dies erm√∂glicht Entwicklern den Bau leistungsstarker Anwendungen, bei denen LLMs auf Echtzeitinformationen zugreifen und Aktionen ausf√ºhren k√∂nnen.",
    "about_how_title": "### Wie es funktioniert",
    "about_step1": "1. **MCP-Server:** Ein Server stellt 'Tools' (Funktionen) und 'Ressourcen' (Daten) bereit.",
    "about_step2": "2. **MCP-Client:** Diese App verbindet sich mit dem Server und macht Tools dem LLM bekannt.",
    "about_step3": "3. **LLM-Interaktion:** Das LLM entscheidet, Tools zu nutzen, um Benutzeranfragen zu erf√ºllen.",
    "about_step4": "4. **Antwort:** Das LLM nutzt die Daten, um eine Antwort zu generieren.",
    
    "explorer_title": "Interaktiver Tool-Explorer",
    "explorer_intro": "Hier sehen Sie die Tools, die vom MCP-Server verf√ºgbar sind.",
    "explorer_error": "Konnte Tools nicht abrufen. L√§uft der Server?",
    "tool_name": "Tool Name",
    "tool_desc": "Beschreibung",

    "tutorial_title": "Gef√ºhrtes Tutorial: Erstellen Sie Ihr erstes Tool",
    "tutorial_intro": "Dieses Tutorial f√ºhrt Sie durch die Erstellung eines neuen MCP-Tools von Grund auf.",
    "tutorial_step1_title": "Schritt 1: Daten zu `db.json` hinzuf√ºgen",
    "tutorial_step1_desc": "Zuerst f√ºgen wir neue Daten zu unserer Datenbankdatei `src/db.json` hinzu. Wir f√ºgen eine Liste von Universit√§tsveranstaltungen hinzu.",
    "tutorial_step1_code_desc": "√ñffnen Sie `src/db.json` und f√ºgen Sie den folgenden `events`-Abschnitt zum JSON-Objekt hinzu:",
    "tutorial_step2_title": "Schritt 2: Definieren des Schemas in `src/schema.ts`",
    "tutorial_step2_desc": "Als N√§chstes m√ºssen wir das Eingabeschema f√ºr unser neues Tool definieren. Dies sagt dem System, welche Art von Argumenten unser Tool akzeptiert.",
    "tutorial_step2_code_desc": "√ñffnen Sie `src/schema.ts` und f√ºgen Sie die folgende Schemadefinition hinzu:",
    "tutorial_step3_title": "Schritt 3: Hinzuf√ºgen der Tool-Logik in `src/index.ts`",
    "tutorial_step3_desc": "Nun zur Kernlogik. Wir definieren das Tool selbst, das die Veranstaltungsdaten aus unserer `db.json`-Datei liest.",
    "tutorial_step3_code_desc": "√ñffnen Sie `src/index.ts` und f√ºgen Sie die folgende Tool-Definition neben den anderen hinzu:",
    "tutorial_step4_title": "Schritt 4: Neustarten und Testen",
    "tutorial_step4_desc": "Um die √Ñnderungen anzuwenden, m√ºssen Sie den MCP-Server neu starten. Stoppen Sie den Server (Strg+C im Terminal, wo Sie `npm start` ausgef√ºhrt haben) und starten Sie ihn erneut.",
    "tutorial_step4_info": "Stellen Sie sicher, dass Sie Ihre √Ñnderungen in allen drei Dateien gespeichert haben, bevor Sie den Server neu starten.",
    "tutorial_step4_test_desc": "Sobald der Server wieder l√§uft, k√∂nnen Sie Ihr neues Tool hier testen!",
    "tutorial_btn_text": "Test 'get_events' Tool",
    "tutorial_btn_success": "Erfolgreich 'get_events'-Tool aufgerufen!",
    "tutorial_btn_warning": "Das 'get_events'-Tool wurde nicht auf dem Server gefunden. Haben Sie ihn nach dem Speichern neu gestartet?",
    "tutorial_error": "Ein Fehler ist aufgetreten: {e}",

    "model_title": "Verst√§ndnis des 'Modells' in MCP",
    "model_intro": "Das 'Modell' im Model Context Protocol bezieht sich auf ein Large Language Model (LLM), wie Googles Gemini oder DeepSeek. Das LLM fungiert als das 'Gehirn' der Anwendung und entscheidet, wann die vom MCP-Server bereitgestellten Tools verwendet werden sollen, um eine Benutzeranfrage zu beantworten.",
    "model_fc_title": "Die Kraft des Function Calling",
    "model_fc_desc": "Moderne LLMs verf√ºgen √ºber eine leistungsstarke Funktion namens **Function Calling** (oder 'Tool Use'). Dies erm√∂glicht es einem Entwickler, dem LLM eine Reihe von Funktionen (unsere MCP-Tools) zu beschreiben. Wenn der Benutzer eine Frage stellt, kann das LLM die Anfrage analysieren und, wenn es eine Funktion f√ºr notwendig h√§lt, um sie zu beantworten, ein spezielles 'Function Call'-Objekt ausgeben. Dieses Objekt enth√§lt: Den `Namen` der Funktion, die es aufrufen m√∂chte. Die `Argumente`, die es an diese Funktion √ºbergeben m√∂chte. Unser Anwendungscode empf√§ngt dieses Objekt, f√ºhrt das entsprechende Tool auf dem MCP-Server aus und sendet das Ergebnis an das LLM zur√ºck. Das LLM verwendet dieses Ergebnis dann, um seine endg√ºltige, datengest√ºtzte Antwort f√ºr den Benutzer zu formulieren.",
    "model_app_approach": "Ansatz 1: Der Zwei-Schritt-Prompt (Python Client)",
    "model_app_desc": "Die Hauptanwendung in diesem Prototyp (`client/frontend.py`) verwendet eine 'Zwei-Schritt'-Prompting-Strategie. Dies ist ein g√§ngiges und effektives Muster bei der Verwendung von Modellen, die m√∂glicherweise kein robustes, eingebautes Function Calling haben, oder wenn man mehr Kontrolle √ºber die Routing-Logik haben m√∂chte.",
    "model_router_title": "Schritt 1: Der Router",
    "model_router_desc": "Zuerst senden wir einen 'Router-Prompt' an das LLM. Dieser Prompt beschreibt die verf√ºgbaren Tools und bittet das LLM zu entscheiden, welches basierend auf der Benutzeranfrage verwendet werden soll. Es wird angewiesen, *nur* mit einem JSON-Objekt zu antworten, das seine Entscheidung darstellt.",
    "model_formatter_title": "Schritt 2: Der Formatierer",
    "model_formatter_desc": "Nachdem wir das Tool ausgef√ºhrt und die Daten erhalten haben, senden wir einen *zweiten* Prompt an das LLM. Dieser 'Formatierer-Prompt' enth√§lt die urspr√ºngliche Anfrage und die Daten, die wir gerade abgerufen haben. Er bittet das LLM, eine benutzerfreundliche Antwort zu formulieren und, was wichtig ist, einen 'Lernpfad' f√ºr den n√§chsten Schritt vorzuschlagen.",
    "model_ts_approach": "Ansatz 2: Natives Function Calling (TypeScript Client)",
    "model_ts_desc": "Die Datei `src/client.ts` demonstriert einen direkteren Ansatz unter Verwendung der nativen Function-Calling-Funktion der Google Gemini API. Dieser Ansatz ist oft sauberer und erfordert weniger Prompt-Engineering. Bei dieser Methode stellen wir die Tool-Schemata direkt dem Modell zur Verf√ºgung, wenn wir die Chat-Sitzung initialisieren. Das Modell √ºbernimmt dann den Entscheidungsprozess intern.",
    "model_info_note": "Beide Ans√§tze sind valide und haben ihre eigenen Vor- und Nachteile. Der Zwei-Schritt-Ansatz gibt Ihnen mehr feink√∂rnige Kontrolle, w√§hrend der native Function-Calling-Ansatz oft einfacher zu implementieren ist.",

    "exercise_title": "√úbung: Erweitern des Lernpfads",
    "exercise_intro": "Der 'Lernpfad' ist das Feature, bei dem der Assistent dem Benutzer den n√§chsten logischen Schritt vorschl√§gt.",
    "exercise_current_prompt": "Aktueller Formatierer-Prompt",
    "exercise_your_turn": "Du bist dran!",
    "exercise_task": "Versuchen wir nun, den Lernpfad zu erweitern. Zum Beispiel, was ist, wenn der Benutzer gerade das `get_events`-Tool verwendet hat, das du erstellt hast? Was w√§re ein guter n√§chster Schritt, den man vorschlagen k√∂nnte? √Ñndere den Prompt unten. Du kannst eine neue Regel zum Abschnitt `LERNPFAD` hinzuf√ºgen. Zum Beispiel: `- Wenn sie Events gepr√ºft haben -> \"M√∂chtest du ein Ticket buchen?\"`",
    "exercise_custom_prompt_label": "Dein eigener Formatierer-Prompt:",
    "exercise_save_btn": "Speichern und eigenen Prompt anwenden",
    "exercise_save_success": "Dein eigener Prompt wurde gespeichert! Gehe jetzt zur 'App'-Seite und versuche eine Anfrage, die deine neue Regel ausl√∂sen w√ºrde.",
    "exercise_active_info": "Ein eigener Formatierer-Prompt ist derzeit aktiv.",
    "exercise_reset_btn": "Auf Standard-Prompt zur√ºcksetzen",
    "exercise_reset_success": "Der Formatierer-Prompt wurde auf den Standard zur√ºckgesetzt."
}
